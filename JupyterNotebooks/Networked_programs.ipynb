{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 12 Networked Programs\n",
    "\n",
    "Chapter 12 of Py4E gets into networked programs, primarily using HyperText Transfer Protocol (http).\n",
    "\n",
    "## Sockets\n",
    "\n",
    "This chapter introduces the concept of a **socket**. This is something that will continue to come up, so is important to understand. \n",
    "\n",
    "In some ways, a socket is like a file handle--it provides access to the information, not the information itself. In other ways a socket is different in that it provides two way communication for sending and recieving information.\n",
    "\n",
    "Here, we'll use sockets to connect to a web server and get the contents of a web page. Different from opening a file on disk, more coordination is needed between your computer and the web server to transmit data, confirm reciept of data, etc. Later, we'll use sockets to connect to databases. And again coordination and established protocols for sending and receiving data come into play.\n",
    "\n",
    "### Protocols\n",
    "\n",
    "As described in the text, in order for two computers to communicate successfully, they need to be following some protocol, or established proceedures for communicating. HTTP is one protocol. We looked briefly at the SFTP (Secure File Transfer Protocol) earlier in the semester for transfering files from our computers to the cluster. Other protocols you are familiar with include Internet Message Access Protocol (IMAP), Post Office Protocol version 3 (POP3) and Simple Mail Transfer Protocol (SMTP) all used for email systems.\n",
    "\n",
    "There are many protocols for different types of communications, the important thing is that you need to establish which protocol is being used and follow the specifications of that protocol.\n",
    "\n",
    "Here's the code for socket1.py (remember these are in the code3 directory of the repository)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP/1.1 200 OK\r\n",
      "Date: Wed, 10 Oct 2018 16:49:07 GMT\r\n",
      "Server: Apache/2.4.18 (Ubuntu)\r\n",
      "Last-Modified: Sat, 13 May 2017 11:22:22 GMT\r\n",
      "ETag: \"a7-54f6609245537\"\r\n",
      "Accept-Ranges: bytes\r\n",
      "Content-Length: 167\r\n",
      "Cache-Control: max-age=0, no-cache, no-store, must-revalidate\r\n",
      "Pragma: no-cache\r\n",
      "Expires: Wed, 11 Jan 1984 05:00:00 GMT\r\n",
      "Connection: close\r\n",
      "Content-Type: text/plain\r\n",
      "\r\n",
      "But soft what light through yonder window breaks\n",
      "It is the east and Juliet is the sun\n",
      "Arise fair sun and kill the envious moon\n",
      "Who is already sick and pale with grief\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "mysock.connect(('data.pr4e.org', 80))\n",
    "cmd = 'GET http://data.pr4e.org/romeo.txt HTTP/1.0\\r\\n\\r\\n'.encode()\n",
    "mysock.send(cmd)\n",
    "\n",
    "while True:\n",
    "    data = mysock.recv(512) \n",
    "    if (len(data) < 1):\n",
    "        break\n",
    "    print(data.decode(),end='')\n",
    "\n",
    "mysock.close()\n",
    "\n",
    "# Code: http://www.py4e.com/code3/socket1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the urljpeg.py script and info on that in the text.\n",
    "\n",
    "## Section 12.4 Using `urllib`\n",
    "\n",
    "The `urllib` module makes this a bit easier. The socket1.py script above can be simplified to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But soft what light through yonder window breaks\n",
      "It is the east and Juliet is the sun\n",
      "Arise fair sun and kill the envious moon\n",
      "Who is already sick and pale with grief\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "fhand = urllib.request.urlopen('http://data.pr4e.org/romeo.txt') \n",
    "for line in fhand:\n",
    "    print(line.decode().strip())\n",
    "# Code: http://www.py4e.com/code3/urllib1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `urllib` sits between the script and the socket to make an opened socket look just like a file handle and we can treat the web page as a local file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 12.6 Parsing HTML\n",
    "\n",
    "We could write our own scripts to parse HTML looking for information. The example here is using a regular expression to search for a link and make a list of links on a page.\n",
    "\n",
    "Run urlregex.py on some site, google.com for example:\n",
    "\n",
    "```bash\n",
    "[magitz@login2 code3]$ python3 urlregex.py\n",
    "Enter - http://google.com\n",
    "http://www.google.com/imghp?hl=en&tab=wi\n",
    "http://maps.google.com/maps?hl=en&tab=wl\n",
    "https://play.google.com/?hl=en&tab=w8\n",
    "http://www.youtube.com/?gl=US&tab=w1\n",
    "http://news.google.com/nwshp?hl=en&tab=wn\n",
    "https://mail.google.com/mail/?tab=wm\n",
    "https://drive.google.com/?tab=wo\n",
    "https://www.google.com/intl/en/options/\n",
    "http://www.google.com/history/optout?hl=en\n",
    "https://accounts.google.com/ServiceLogin?hl=en&passive=true&continue=http://www.google.com/\n",
    "https://plus.google.com/116899029375914044550\n",
    "[magitz@login2 code3]$ \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, not all web pages follow HTML guidelines totally and sometimes pages can be really hard to parse. Tags can be upper and lower case, some closing tags are optional, etc. It can quickly get quite complex.\n",
    "\n",
    "## Section 12.7 Parsing HTML using BeautifulSoup\n",
    "\n",
    "As I mentioned earlier, whatever you are trying to do, look for a module to make your life easier. If you need to parse HTML, don't start trying to write your own script to do it, look at available modules. One is `BeautifulSoup` available from crummy.com--some people sure have fun!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter - http://google.com\n",
      "http://www.google.com/imghp?hl=en&tab=wi\n",
      "http://maps.google.com/maps?hl=en&tab=wl\n",
      "https://play.google.com/?hl=en&tab=w8\n",
      "http://www.youtube.com/?gl=US&tab=w1\n",
      "http://news.google.com/nwshp?hl=en&tab=wn\n",
      "https://mail.google.com/mail/?tab=wm\n",
      "https://drive.google.com/?tab=wo\n",
      "https://www.google.com/intl/en/options/\n",
      "http://www.google.com/history/optout?hl=en\n",
      "/preferences?hl=en\n",
      "https://accounts.google.com/ServiceLogin?hl=en&passive=true&continue=http://www.google.com/\n",
      "/advanced_search?hl=en&authuser=0\n",
      "/language_tools?hl=en&authuser=0\n",
      "/intl/en/ads/\n",
      "/services/\n",
      "https://plus.google.com/116899029375914044550\n",
      "/intl/en/about.html\n",
      "/intl/en/policies/privacy/\n",
      "/intl/en/policies/terms/\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = input('Enter - ')\n",
    "html = urllib.request.urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('a') \n",
    "for tag in tags:\n",
    "    print(tag.get('href', None))\n",
    "\n",
    "# Code: http://www.py4e.com/code3/urllinks.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional info\n",
    "\n",
    "The script `urllink2.py` shows some of the additional information you can get with BeautifulSoup.\n",
    "\n",
    "Section 12.8 gets into using urllib to download binary files--anything other than text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
